version: '2.3'

services:

  tf2svg1:    # tf serving CPU, without GPU
    #build:
    #  context: ./covid19/covid19_models
    image: tensorflow/serving
    hostname: tf2svg1
    environment:
      - MODEL_NAME=covid19
    restart: on-failure
    ports:
      - 8511:8501 # tensorflow servring port
      - 8510:8500 # grpc port
    volumes:
      - ./covid19/head-base-covid:/models/covid19

  fastapi:
    build:
      context: ./covid-fastapi
    image: ai-service-fastapi:0.2.0
    hostname: fastapi
    environment:
      - MODEL2_API_URL_C = test
    restart: on-failure
    ports:
      - 8056:8000
      - 8057:8000 
    volumes:
      - ./covid-fastapi:/app  # tf2 serving convention

  fastapi2:
    build:
      context: ./covid-fastapi
    image: ai-service-fastapi:0.2.0
    hostname: fastapi2
    environment:
      - MODEL2_API_URL_C = test
    restart: on-failure
    ports:
      - 8058:8000 
    volumes:
      - ./covid-fastapi:/app  # tf2 serving convention

  # flask:    # flask server for covid19 service web pages and APIs, with backend tensorflow servering
  #   build:
  #     context: ./covid19
  #   image: ai-service-flask:0.1.0
  #   hostname: flask
  #   restart: on-failure
  #   ports:
  #     - 8051:5000 # tensorflow servring port
  #     - 8052:5000 
  #   volumes:
  #     - ./covid19:/app  # tf2 serving convention

  lb:
    build:
      context: ./haproxy
    image: 'haproxy:0.0.1'
    volumes:
      - ./haproxy:/haproxy-override
    links:
      # - ${RUN_APP}
      - fastapi
      - fastapi2
    ports:
      - 8088:8088
    expose:
      - 8088
